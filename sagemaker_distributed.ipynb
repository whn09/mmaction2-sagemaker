{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training MMAction3 Mask-RCNN Model on Sagemaker Distributed Cluster\n",
    "\n",
    "## Motivation\n",
    "[MMDetection](https://github.com/open-mmlab/mmdetection) is a popular open-source Deep Learning framework focused on Computer Vision models and use cases. MMDetection provides to higher level APIs for model training and inference. It demonstrates [state-of-the-art benchmarks](https://github.com/open-mmlab/mmdetection#benchmark-and-model-zoo) for variety of model architecture and extensive Model Zoo.\n",
    "\n",
    "In this notebook, we will build a custom training container with MMdetection library and then train Mask-RCNN model from scratch on [COCO2017 dataset](https://cocodataset.org/#home) using Sagemaker distributed [training feature](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html) in order to reduce training time.\n",
    "\n",
    "### Preconditions\n",
    "- To execute this notebook, you will need to have COCO 2017 training and validation datasets uploaded to S3 bucket available for Amazon Sagemaker service.\n",
    "\n",
    "\n",
    "## Building Training Container\n",
    "\n",
    "Amazon Sagemaker allows to BYO containers for training, data processing, and inference. In our case, we need to build custom training container which will be pushed to your AWS account [ECR service](https://aws.amazon.com/ecr/). \n",
    "\n",
    "For this, we need to login to public ECR with Sagemaker base images and private ECR reposity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, boto3\n",
    "\n",
    "session = sagemaker.Session()\n",
    "region = session.boto_region_name\n",
    "account = boto3.client('sts').get_caller_identity().get('Account')\n",
    "bucket = session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "# login to Sagemaker ECR with Deep Learning Containers\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin 763104351884.dkr.ecr.{region}.amazonaws.com\n",
    "# login to your private ECR\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin {account}.dkr.ecr.{region}.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let review training container:\n",
    "- use Sagemaker PyTorch 1.5.0 container as base image;\n",
    "- install latest version of Pytorch libraries and MMdetection dependencies;\n",
    "- build MMDetection from sources;\n",
    "- configure Sagemaker env variables, specifically, what script to use at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pygmentize -l docker Dockerfile.training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "Next, we build and push custom training container to private ECR\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Sending build context to Docker daemon  37.05MB\n",
      "Step 1/13 : FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.5.0-gpu-py36-cu101-ubuntu16.04\n",
      " ---> 47cd15520b75\n",
      "Step 2/13 : LABEL author=\"vadimd@amazon.com\"\n",
      " ---> Using cache\n",
      " ---> 78da0851d3c4\n",
      "Step 3/13 : WORKDIR /opt/ml/code\n",
      " ---> Using cache\n",
      " ---> 07bd9a0de06d\n",
      "Step 4/13 : RUN pip install --upgrade --force-reinstall  torch torchvision cython\n",
      " ---> Using cache\n",
      " ---> b13c99508ae7\n",
      "Step 5/13 : RUN pip install mmcv-full==latest+torch1.7.0+cu101 -f https://download.openmmlab.com/mmcv/dist/index.html\n",
      " ---> Using cache\n",
      " ---> 7e525985aced\n",
      "Step 6/13 : RUN git clone https://github.com/open-mmlab/mmaction2.git\n",
      " ---> Using cache\n",
      " ---> 725bfb4c2796\n",
      "Step 7/13 : RUN cd mmaction2/ &&     pip install -r requirements/build.txt &&     pip install -e .\n",
      " ---> Using cache\n",
      " ---> f1734add8d2e\n",
      "Step 8/13 : ENV MKL_THREADING_LAYER GNU\n",
      " ---> Using cache\n",
      " ---> e7d7a3d6954f\n",
      "Step 9/13 : ENV MMACTION2 /opt/ml/code/mmaction2\n",
      " ---> Running in 7c58fac8ebaf\n",
      "Removing intermediate container 7c58fac8ebaf\n",
      " ---> 667844265ee0\n",
      "Step 10/13 : COPY container_training /opt/ml/code\n",
      " ---> 32491afa54b2\n",
      "Step 11/13 : ENV SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code\n",
      " ---> Running in 631ae665c312\n",
      "Removing intermediate container 631ae665c312\n",
      " ---> 6cd8bb176ac9\n",
      "Step 12/13 : ENV SAGEMAKER_PROGRAM mmaction2_train.py\n",
      " ---> Running in 47487cd0d946\n",
      "Removing intermediate container 47487cd0d946\n",
      " ---> 26a15ec8db09\n",
      "Step 13/13 : WORKDIR /\n",
      " ---> Running in 2a8357ae63e3\n",
      "Removing intermediate container 2a8357ae63e3\n",
      " ---> fe8a6f80ba42\n",
      "Successfully built fe8a6f80ba42\n",
      "Successfully tagged mmaction2-training:latest\n",
      "The push refers to repository [579019700964.dkr.ecr.us-east-1.amazonaws.com/mmaction2-training]\n",
      "\n",
      "\u001b[1Be9556613: Preparing \n",
      "\u001b[1B38688bcf: Preparing \n",
      "\u001b[1B2961137e: Preparing \n",
      "\u001b[1B20bac855: Preparing \n",
      "\u001b[1B1aef3cb1: Preparing \n",
      "\u001b[1B87d2d613: Preparing \n",
      "\u001b[1Bdc6eccf1: Preparing \n",
      "\u001b[1Be8cb8ead: Preparing \n",
      "\u001b[1B18b6f784: Preparing \n",
      "\u001b[1Bbe96fc82: Preparing \n",
      "\u001b[1B2b332e53: Preparing \n",
      "\u001b[1Bc6e1c93f: Preparing \n",
      "\u001b[1B8af0cced: Preparing \n",
      "\u001b[1Ba1e058e6: Preparing \n",
      "\u001b[1Ba7e2d141: Preparing \n",
      "\u001b[1B891256c7: Preparing \n",
      "\u001b[1B4275da12: Preparing \n",
      "\u001b[1B8601ef26: Preparing \n",
      "\u001b[1B03245374: Preparing \n",
      "\u001b[1Be3aaa392: Preparing \n",
      "\u001b[1B09ca2db2: Preparing \n",
      "\u001b[1B3589d5b4: Preparing \n",
      "\u001b[1B3141886c: Preparing \n",
      "\u001b[19B7d2d613: Waiting g \n",
      "\u001b[16Be96fc82: Waiting g \n",
      "\u001b[20Bc6eccf1: Waiting g \n",
      "\u001b[1B74e50f52: Preparing \n",
      "\u001b[21B8cb8ead: Waiting g \n",
      "\u001b[19Bb332e53: Waiting g \n",
      "\u001b[1B9683cb41: Preparing \n",
      "\u001b[23B8b6f784: Waiting g \n",
      "\u001b[21B6e1c93f: Waiting g \n",
      "\u001b[21Baf0cced: Waiting g \n",
      "\u001b[2Be637fbff: Layer already exists 7kB4A\u001b[2K\u001b[24A\u001b[2K\u001b[20A\u001b[2K\u001b[21A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[3A\u001b[2Klatest: digest: sha256:ae741f9fde9426eb42678d044d18d973c1f1aa81206772a6170fe5aee6b8afd9 size: 7474\n"
     ]
    }
   ],
   "source": [
    "! ./build_and_push.sh mmaction2-training latest Dockerfile.training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training script\n",
    "\n",
    "At training time, Sagemaker executes training script defined in `SAGEMAKER_PROGRAM` variable. In our case, this script does following\n",
    "- parses user parameters passed via Sagemaker Hyperparameter dictionary;\n",
    "- based on parameters constructs launch command;\n",
    "- uses `torch.distributed.launch` utility to launch distributed training;\n",
    "- uses MMDetection `tools/train.py` to configure trianing process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pygmentize container_training/mmaction2_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Sagemaker Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "prefix_input = 'mmaction2-input'\n",
    "prefix_output = 'mmaction2-ouput'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = \"mmaction2-training\" # your container name\n",
    "tag = \"latest\"\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/{}:{}'.format(account, region, container, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm parameters\n",
    "\n",
    "hyperparameters = {\n",
    "    \"config-file\" : \"configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py\", # config path is relative to MMDetection root directory\n",
    "    \"dataset\" : \"kinetics400_tiny\",\n",
    "    \"auto-scale\" : \"false\", # whether to scale LR and Warm Up time\n",
    "    \"validate\" : \"true\", # whether to run validation after training is done\n",
    "    \n",
    "    # 'options' allows to override individual config values\n",
    "    \"options\" : \"total_epochs=1; optimizer.lr=0.08; evaluation.gpu_collect=True\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sagemaker will parse metrics from STDOUT and store/visualize them as part of training job\n",
    "metrics = [\n",
    "    {\n",
    "        \"Name\": \"top_k_accuracy\",\n",
    "        \"Regex\": \".*top_k_accuracy:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"mean_class_accuracy\",\n",
    "        \"Regex\": \".*mean_class_accuracy:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "Execute cell below to start training on Sagemaker.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘kinetics400_tiny.zip*’: No such file or directory\n",
      "--2020-12-10 12:43:21--  https://download.openmmlab.com/mmaction/kinetics400_tiny.zip\n",
      "Resolving download.openmmlab.com (download.openmmlab.com)... 47.252.96.35\n",
      "Connecting to download.openmmlab.com (download.openmmlab.com)|47.252.96.35|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 18308682 (17M) [application/zip]\n",
      "Saving to: ‘kinetics400_tiny.zip’\n",
      "\n",
      "kinetics400_tiny.zi 100%[===================>]  17.46M  8.76MB/s    in 2.0s    \n",
      "\n",
      "2020-12-10 12:43:24 (8.76 MB/s) - ‘kinetics400_tiny.zip’ saved [18308682/18308682]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download, decompress the data\n",
    "!rm kinetics400_tiny.zip*\n",
    "!rm -rf kinetics400_tiny\n",
    "!wget https://download.openmmlab.com/mmaction/kinetics400_tiny.zip\n",
    "!unzip kinetics400_tiny.zip > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded plugins: dkms-build-requires, priorities, update-motd, upgrade-helper,\n",
      "              : versionlock\n",
      "No packages marked for update\n",
      "Loaded plugins: dkms-build-requires, priorities, update-motd, upgrade-helper,\n",
      "              : versionlock\n",
      "Package tree-1.6.0-5.8.amzn1.x86_64 already installed and latest version\n",
      "Nothing to do\n",
      "\u001b[01;34mkinetics400_tiny\u001b[00m\n",
      "├── kinetics_tiny_train_video.txt\n",
      "├── kinetics_tiny_val_video.txt\n",
      "├── \u001b[01;34mtrain\u001b[00m\n",
      "│   ├── \u001b[01;35m27_CSXByd3s.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35m34XczvTaRiI.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mA-wiliK50Zw.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mD32_1gwq35E.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mD92m0HsHjcQ.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mDbX8mPslRXg.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mFMlSTTpN3VY.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mh10B9SVE-nk.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mh2YqqUhnR34.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35miRuyZSKhHRg.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mIyfILH9lBRo.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mkFC3KY2bOP8.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mLvcFDgCAXQs.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mO46YA8tI530.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35moMrZaozOvdQ.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35moXy-e_P_cAI.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mP5M-hAts7MQ.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mphDqGd0NKoo.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mPnOe3GZRVX8.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mR8HXQkdgKWA.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mRqnKtCEoEcA.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35msoEcZZsBmDs.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mTkkZPZHbAKA.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mT_TMNGzVrDk.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mWaS0qwP46Us.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mWh_YPQdH1Zg.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mWWP5HZJsg-o.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mxGY2dP0YUjA.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35myLC9CtWU5ws.mp4\u001b[00m\n",
      "│   └── \u001b[01;35mZQV4U2KQ370.mp4\u001b[00m\n",
      "└── \u001b[01;34mval\u001b[00m\n",
      "    ├── \u001b[01;35m0pVGiAU6XEA.mp4\u001b[00m\n",
      "    ├── \u001b[01;35mAQrbRSnRt8M.mp4\u001b[00m\n",
      "    ├── \u001b[01;35mb6Q_b7vgc7Q.mp4\u001b[00m\n",
      "    ├── \u001b[01;35mddvJ6-faICE.mp4\u001b[00m\n",
      "    ├── \u001b[01;35mIcLztCtvhb8.mp4\u001b[00m\n",
      "    ├── \u001b[01;35mik4BW3-SCts.mp4\u001b[00m\n",
      "    ├── \u001b[01;35mjqRrH30V0k4.mp4\u001b[00m\n",
      "    ├── \u001b[01;35mSU_x2LQqSLs.mp4\u001b[00m\n",
      "    ├── \u001b[01;35mu4Rm6srmIS8.mp4\u001b[00m\n",
      "    └── \u001b[01;35my5Iu7XkTqV0.mp4\u001b[00m\n",
      "\n",
      "2 directories, 42 files\n"
     ]
    }
   ],
   "source": [
    "# Check the directory structure of the tiny data\n",
    "\n",
    "# Install tree first\n",
    "!sudo yum update -y && sudo yum install -y tree\n",
    "!tree kinetics400_tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D32_1gwq35E.mp4 0\n",
      "iRuyZSKhHRg.mp4 1\n",
      "oXy-e_P_cAI.mp4 0\n",
      "34XczvTaRiI.mp4 1\n",
      "h2YqqUhnR34.mp4 0\n",
      "O46YA8tI530.mp4 0\n",
      "kFC3KY2bOP8.mp4 1\n",
      "WWP5HZJsg-o.mp4 1\n",
      "phDqGd0NKoo.mp4 1\n",
      "yLC9CtWU5ws.mp4 0\n",
      "27_CSXByd3s.mp4 1\n",
      "IyfILH9lBRo.mp4 1\n",
      "T_TMNGzVrDk.mp4 1\n",
      "TkkZPZHbAKA.mp4 0\n",
      "PnOe3GZRVX8.mp4 1\n",
      "soEcZZsBmDs.mp4 1\n",
      "FMlSTTpN3VY.mp4 1\n",
      "WaS0qwP46Us.mp4 0\n",
      "A-wiliK50Zw.mp4 1\n",
      "oMrZaozOvdQ.mp4 1\n",
      "ZQV4U2KQ370.mp4 0\n",
      "DbX8mPslRXg.mp4 1\n",
      "h10B9SVE-nk.mp4 1\n",
      "P5M-hAts7MQ.mp4 0\n",
      "R8HXQkdgKWA.mp4 0\n",
      "D92m0HsHjcQ.mp4 0\n",
      "RqnKtCEoEcA.mp4 0\n",
      "LvcFDgCAXQs.mp4 0\n",
      "xGY2dP0YUjA.mp4 0\n",
      "Wh_YPQdH1Zg.mp4 0\n"
     ]
    }
   ],
   "source": [
    "# After downloading the data, we need to check the annotation format\n",
    "!cat kinetics400_tiny/kinetics_tiny_train_video.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: kinetics400_tiny/kinetics_tiny_train_video.txt to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/kinetics_tiny_train_video.txt\n",
      "upload: kinetics400_tiny/kinetics_tiny_val_video.txt to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/kinetics_tiny_val_video.txt\n",
      "upload: kinetics400_tiny/train/LvcFDgCAXQs.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/LvcFDgCAXQs.mp4\n",
      "upload: kinetics400_tiny/train/A-wiliK50Zw.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/A-wiliK50Zw.mp4\n",
      "upload: kinetics400_tiny/train/DbX8mPslRXg.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/DbX8mPslRXg.mp4\n",
      "upload: kinetics400_tiny/train/IyfILH9lBRo.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/IyfILH9lBRo.mp4\n",
      "upload: kinetics400_tiny/train/D92m0HsHjcQ.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/D92m0HsHjcQ.mp4\n",
      "upload: kinetics400_tiny/train/27_CSXByd3s.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/27_CSXByd3s.mp4\n",
      "upload: kinetics400_tiny/train/FMlSTTpN3VY.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/FMlSTTpN3VY.mp4\n",
      "upload: kinetics400_tiny/train/34XczvTaRiI.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/34XczvTaRiI.mp4\n",
      "upload: kinetics400_tiny/train/D32_1gwq35E.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/D32_1gwq35E.mp4\n",
      "upload: kinetics400_tiny/train/PnOe3GZRVX8.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/PnOe3GZRVX8.mp4\n",
      "upload: kinetics400_tiny/train/WWP5HZJsg-o.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/WWP5HZJsg-o.mp4\n",
      "upload: kinetics400_tiny/train/T_TMNGzVrDk.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/T_TMNGzVrDk.mp4\n",
      "upload: kinetics400_tiny/train/RqnKtCEoEcA.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/RqnKtCEoEcA.mp4\n",
      "upload: kinetics400_tiny/train/ZQV4U2KQ370.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/ZQV4U2KQ370.mp4\n",
      "upload: kinetics400_tiny/train/WaS0qwP46Us.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/WaS0qwP46Us.mp4\n",
      "upload: kinetics400_tiny/train/R8HXQkdgKWA.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/R8HXQkdgKWA.mp4\n",
      "upload: kinetics400_tiny/train/O46YA8tI530.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/O46YA8tI530.mp4\n",
      "upload: kinetics400_tiny/train/TkkZPZHbAKA.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/TkkZPZHbAKA.mp4\n",
      "upload: kinetics400_tiny/train/Wh_YPQdH1Zg.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/Wh_YPQdH1Zg.mp4\n",
      "upload: kinetics400_tiny/train/P5M-hAts7MQ.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/P5M-hAts7MQ.mp4\n",
      "upload: kinetics400_tiny/train/h10B9SVE-nk.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/h10B9SVE-nk.mp4\n",
      "upload: kinetics400_tiny/train/oXy-e_P_cAI.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/oXy-e_P_cAI.mp4\n",
      "upload: kinetics400_tiny/train/kFC3KY2bOP8.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/kFC3KY2bOP8.mp4\n",
      "upload: kinetics400_tiny/train/iRuyZSKhHRg.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/iRuyZSKhHRg.mp4\n",
      "upload: kinetics400_tiny/train/h2YqqUhnR34.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/h2YqqUhnR34.mp4\n",
      "upload: kinetics400_tiny/train/yLC9CtWU5ws.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/yLC9CtWU5ws.mp4\n",
      "upload: kinetics400_tiny/train/xGY2dP0YUjA.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/xGY2dP0YUjA.mp4\n",
      "upload: kinetics400_tiny/train/phDqGd0NKoo.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/phDqGd0NKoo.mp4\n",
      "upload: kinetics400_tiny/train/oMrZaozOvdQ.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/oMrZaozOvdQ.mp4\n",
      "upload: kinetics400_tiny/val/IcLztCtvhb8.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/val/IcLztCtvhb8.mp4\n",
      "upload: kinetics400_tiny/val/SU_x2LQqSLs.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/val/SU_x2LQqSLs.mp4\n",
      "upload: kinetics400_tiny/train/soEcZZsBmDs.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/train/soEcZZsBmDs.mp4\n",
      "upload: kinetics400_tiny/val/b6Q_b7vgc7Q.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/val/b6Q_b7vgc7Q.mp4\n",
      "upload: kinetics400_tiny/val/AQrbRSnRt8M.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/val/AQrbRSnRt8M.mp4\n",
      "upload: kinetics400_tiny/val/0pVGiAU6XEA.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/val/0pVGiAU6XEA.mp4\n",
      "upload: kinetics400_tiny/val/u4Rm6srmIS8.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/val/u4Rm6srmIS8.mp4\n",
      "upload: kinetics400_tiny/val/ik4BW3-SCts.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/val/ik4BW3-SCts.mp4\n",
      "upload: kinetics400_tiny/val/y5Iu7XkTqV0.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/val/y5Iu7XkTqV0.mp4\n",
      "upload: kinetics400_tiny/val/ddvJ6-faICE.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/val/ddvJ6-faICE.mp4\n",
      "upload: kinetics400_tiny/val/jqRrH30V0k4.mp4 to s3://sagemaker-us-east-1-579019700964/kinetics400_tiny/val/jqRrH30V0k4.mp4\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive kinetics400_tiny s3://$bucket/kinetics400_tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_volume_size has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-10 13:33:32 Starting - Starting the training job...\n",
      "2020-12-10 13:33:55 Starting - Launching requested ML instancesProfilerReport-1607607212: InProgress\n",
      "............\n",
      "2020-12-10 13:35:57 Starting - Preparing the instances for training......\n",
      "2020-12-10 13:36:58 Downloading - Downloading input data......\n",
      "2020-12-10 13:37:59 Training - Downloading the training image..................\n",
      "2020-12-10 13:41:01 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2020-12-10 13:40:58,766 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2020-12-10 13:40:58,767 sagemaker-containers INFO     Failed to parse hyperparameter config-file value configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2020-12-10 13:40:58,767 sagemaker-containers INFO     Failed to parse hyperparameter options value total_epochs=1; optimizer.lr=0.08; evaluation.gpu_collect=True to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2020-12-10 13:40:58,767 sagemaker-containers INFO     Failed to parse hyperparameter dataset value kinetics400_tiny to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2020-12-10 13:40:58,810 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-12-10 13:41:00,498 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-12-10 13:41:00,499 sagemaker-containers INFO     Failed to parse hyperparameter config-file value configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-12-10 13:41:00,499 sagemaker-containers INFO     Failed to parse hyperparameter options value total_epochs=1; optimizer.lr=0.08; evaluation.gpu_collect=True to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-12-10 13:41:00,499 sagemaker-containers INFO     Failed to parse hyperparameter dataset value kinetics400_tiny to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-12-10 13:41:00,542 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-12-10 13:41:03,578 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-12-10 13:41:03,981 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-12-10 13:41:03,981 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-12-10 13:41:03,981 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-12-10 13:41:03,981 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmpb2vq5wfj/module_dir\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[35m2020-12-10 13:41:05,037 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2020-12-10 13:41:05,446 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[35mGenerating setup.py\u001b[0m\n",
      "\u001b[35m2020-12-10 13:41:05,446 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[35m2020-12-10 13:41:05,446 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[35m2020-12-10 13:41:05,446 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python -m pip install . \u001b[0m\n",
      "\u001b[35mProcessing /tmp/tmp18tx_0zz/module_dir\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=44417781 sha256=a00e25c1df6cabbfbcdedfa4451fb53eb66c3d672086c21c0762fac261e1588d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-tu8ed05g/wheels/9f/54/b2/126e7a2189cc9305f9c5675d2b993fd27c782e038842e8ccc4\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[34mInstalling collected packages: default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.1; however, version 20.3.1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[35m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=44417781 sha256=945b98011c19604080313f26104c014663c064cb012a0f10a207b3ebfe8db3e9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-6f0lqss_/wheels/28/c1/73/791831f9e8166e938839c092fc3a47737b22dc76ec55e14dd2\u001b[0m\n",
      "\u001b[35mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[34m2020-12-10 13:41:09,648 sagemaker-containers INFO     Failed to parse hyperparameter config-file value configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-12-10 13:41:09,649 sagemaker-containers INFO     Failed to parse hyperparameter options value total_epochs=1; optimizer.lr=0.08; evaluation.gpu_collect=True to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-12-10 13:41:09,649 sagemaker-containers INFO     Failed to parse hyperparameter dataset value kinetics400_tiny to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-12-10 13:41:09,664 sagemaker-containers INFO     Failed to parse hyperparameter config-file value configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-12-10 13:41:09,664 sagemaker-containers INFO     Failed to parse hyperparameter options value total_epochs=1; optimizer.lr=0.08; evaluation.gpu_collect=True to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-12-10 13:41:09,664 sagemaker-containers INFO     Failed to parse hyperparameter dataset value kinetics400_tiny to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-12-10 13:41:09,709 sagemaker-containers INFO     Failed to parse hyperparameter config-file value configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-12-10 13:41:09,709 sagemaker-containers INFO     Failed to parse hyperparameter options value total_epochs=1; optimizer.lr=0.08; evaluation.gpu_collect=True to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-12-10 13:41:09,709 sagemaker-containers INFO     Failed to parse hyperparameter dataset value kinetics400_tiny to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-12-10 13:41:09,753 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"config-file\": \"configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py\",\n",
      "        \"options\": \"total_epochs=1; optimizer.lr=0.08; evaluation.gpu_collect=True\",\n",
      "        \"dataset\": \"kinetics400_tiny\",\n",
      "        \"auto-scale\": false,\n",
      "        \"validate\": true\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"mmaction2-training-2020-12-10-13-33-32-464\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/code\",\n",
      "    \"module_name\": \"mmaction2_train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 4,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mmaction2_train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"auto-scale\":false,\"config-file\":\"configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py\",\"dataset\":\"kinetics400_tiny\",\"options\":\"total_epochs=1; optimizer.lr=0.08; evaluation.gpu_collect=True\",\"validate\":true}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mmaction2_train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mmaction2_train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=32\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/code\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"auto-scale\":false,\"config-file\":\"configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py\",\"dataset\":\"kinetics400_tiny\",\"options\":\"total_epochs=1; optimizer.lr=0.08; evaluation.gpu_collect=True\",\"validate\":true},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"mmaction2-training-2020-12-10-13-33-32-464\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"mmaction2_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mmaction2_train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--auto-scale\",\"False\",\"--config-file\",\"configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py\",\"--dataset\",\"kinetics400_tiny\",\"--options\",\"total_epochs=1; optimizer.lr=0.08; evaluation.gpu_collect=True\",\"--validate\",\"True\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_CONFIG-FILE=configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIONS=total_epochs=1; optimizer.lr=0.08; evaluation.gpu_collect=True\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET=kinetics400_tiny\u001b[0m\n",
      "\u001b[34mSM_HP_AUTO-SCALE=false\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATE=true\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages:/opt/ml/code/mmaction2\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python mmaction2_train.py --auto-scale False --config-file configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py --dataset kinetics400_tiny --options total_epochs=1; optimizer.lr=0.08; evaluation.gpu_collect=True --validate True\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35mInstalling collected packages: default-user-module-name\u001b[0m\n",
      "\u001b[35mSuccessfully installed default-user-module-name-1.0.0\u001b[0m\n",
      "\u001b[35mWARNING: You are using pip version 20.1; however, version 20.3.1 is available.\u001b[0m\n",
      "\u001b[35mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[35m2020-12-10 13:41:11,157 sagemaker-containers INFO     Failed to parse hyperparameter config-file value configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2020-12-10 13:41:11,158 sagemaker-containers INFO     Failed to parse hyperparameter options value total_epochs=1; optimizer.lr=0.08; evaluation.gpu_collect=True to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2020-12-10 13:41:11,158 sagemaker-containers INFO     Failed to parse hyperparameter dataset value kinetics400_tiny to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2020-12-10 13:41:11,171 sagemaker-containers INFO     Failed to parse hyperparameter config-file value configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2020-12-10 13:41:11,172 sagemaker-containers INFO     Failed to parse hyperparameter options value total_epochs=1; optimizer.lr=0.08; evaluation.gpu_collect=True to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2020-12-10 13:41:11,172 sagemaker-containers INFO     Failed to parse hyperparameter dataset value kinetics400_tiny to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2020-12-10 13:41:11,216 sagemaker-containers INFO     Failed to parse hyperparameter config-file value configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2020-12-10 13:41:11,216 sagemaker-containers INFO     Failed to parse hyperparameter options value total_epochs=1; optimizer.lr=0.08; evaluation.gpu_collect=True to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2020-12-10 13:41:11,216 sagemaker-containers INFO     Failed to parse hyperparameter dataset value kinetics400_tiny to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2020-12-10 13:41:11,259 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[35mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"config-file\": \"configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py\",\n",
      "        \"options\": \"total_epochs=1; optimizer.lr=0.08; evaluation.gpu_collect=True\",\n",
      "        \"dataset\": \"kinetics400_tiny\",\n",
      "        \"auto-scale\": false,\n",
      "        \"validate\": true\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"mmaction2-training-2020-12-10-13-33-32-464\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/code\",\n",
      "    \"module_name\": \"mmaction2_train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 4,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mmaction2_train.py\"\u001b[0m\n",
      "\u001b[35m}\n",
      "\u001b[0m\n",
      "\u001b[35mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"auto-scale\":false,\"config-file\":\"configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py\",\"dataset\":\"kinetics400_tiny\",\"options\":\"total_epochs=1; optimizer.lr=0.08; evaluation.gpu_collect=True\",\"validate\":true}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=mmaction2_train.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=mmaction2_train\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=32\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=/opt/ml/code\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"auto-scale\":false,\"config-file\":\"configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py\",\"dataset\":\"kinetics400_tiny\",\"options\":\"total_epochs=1; optimizer.lr=0.08; evaluation.gpu_collect=True\",\"validate\":true},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"mmaction2-training-2020-12-10-13-33-32-464\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"mmaction2_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mmaction2_train.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--auto-scale\",\"False\",\"--config-file\",\"configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py\",\"--dataset\",\"kinetics400_tiny\",\"--options\",\"total_epochs=1; optimizer.lr=0.08; evaluation.gpu_collect=True\",\"--validate\",\"True\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[35mSM_HP_CONFIG-FILE=configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py\u001b[0m\n",
      "\u001b[35mSM_HP_OPTIONS=total_epochs=1; optimizer.lr=0.08; evaluation.gpu_collect=True\u001b[0m\n",
      "\u001b[35mSM_HP_DATASET=kinetics400_tiny\u001b[0m\n",
      "\u001b[35mSM_HP_AUTO-SCALE=false\u001b[0m\n",
      "\u001b[35mSM_HP_VALIDATE=true\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages:/opt/ml/code/mmaction2\n",
      "\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python mmaction2_train.py --auto-scale False --config-file configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py --dataset kinetics400_tiny --options total_epochs=1; optimizer.lr=0.08; evaluation.gpu_collect=True --validate True\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mStarting training...\u001b[0m\n",
      "\u001b[34mConfig:\u001b[0m\n",
      "\u001b[34mmodel = dict(\n",
      "    type='Recognizer2D',\n",
      "    backbone=dict(\n",
      "        type='ResNet',\n",
      "        pretrained='torchvision://resnet50',\n",
      "        depth=50,\n",
      "        norm_eval=False),\n",
      "    cls_head=dict(\n",
      "        type='TSNHead',\n",
      "        num_classes=2,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        consensus=dict(type='AvgConsensus', dim=1),\n",
      "        dropout_ratio=0.4,\n",
      "        init_std=0.01))\u001b[0m\n",
      "\u001b[34mtrain_cfg = None\u001b[0m\n",
      "\u001b[34mtest_cfg = dict(average_clips=None)\u001b[0m\n",
      "\u001b[34mdataset_type = 'VideoDataset'\u001b[0m\n",
      "\u001b[34mdata_root = '/opt/ml/input/data/training/train/'\u001b[0m\n",
      "\u001b[34mdata_root_val = '/opt/ml/input/data/training/val/'\u001b[0m\n",
      "\u001b[34mann_file_train = '/opt/ml/input/data/training/kinetics_tiny_train_video.txt'\u001b[0m\n",
      "\u001b[34mann_file_val = '/opt/ml/input/data/training/kinetics_tiny_val_video.txt'\u001b[0m\n",
      "\u001b[34mann_file_test = 'kinetics400_tiny/kinetics_tiny_val_video.txt'\u001b[0m\n",
      "\u001b[34mimg_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\u001b[0m\n",
      "\u001b[34mtrain_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=8),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(\n",
      "        type='MultiScaleCrop',\n",
      "        input_size=224,\n",
      "        scales=(1, 0.875, 0.75, 0.66),\n",
      "        random_crop=False,\n",
      "        max_wh_scale_gap=1),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\u001b[0m\n",
      "\u001b[34m]\u001b[0m\n",
      "\u001b[34mval_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=1,\n",
      "        frame_interval=1,\n",
      "        num_clips=8,\n",
      "        test_mode=True),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(type='Flip', flip_ratio=0),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\u001b[0m\n",
      "\u001b[34m]\u001b[0m\n",
      "\u001b[34mtest_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=1,\n",
      "        frame_interval=1,\n",
      "        num_clips=25,\n",
      "        test_mode=True),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(type='Flip', flip_ratio=0),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\u001b[0m\n",
      "\u001b[34m]\u001b[0m\n",
      "\u001b[34mdata = dict(\n",
      "    videos_per_gpu=2,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='/opt/ml/input/data/training/kinetics_tiny_train_video.txt',\n",
      "        data_prefix='/opt/ml/input/data/training/train/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames', clip_len=1, frame_interval=1,\n",
      "                num_clips=8),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(\n",
      "                type='MultiScaleCrop',\n",
      "                input_size=224,\n",
      "                scales=(1, 0.875, 0.75, 0.66),\n",
      "                random_crop=False,\n",
      "                max_wh_scale_gap=1),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='/opt/ml/input/data/training/kinetics_tiny_val_video.txt',\n",
      "        data_prefix='/opt/ml/input/data/training/val/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=1,\n",
      "                frame_interval=1,\n",
      "                num_clips=8,\n",
      "                test_mode=True),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(type='Flip', flip_ratio=0),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ]),\n",
      "    test=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='/opt/ml/input/data/training/kinetics_tiny_val_video.txt',\n",
      "        data_prefix='/opt/ml/input/data/training/val/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=1,\n",
      "                frame_interval=1,\n",
      "                num_clips=25,\n",
      "                test_mode=True),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(type='Flip', flip_ratio=0),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ]))\u001b[0m\n",
      "\u001b[34moptimizer = dict(type='SGD', lr=0.08, momentum=0.9, weight_decay=0.0001)\u001b[0m\n",
      "\u001b[34moptimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\u001b[0m\n",
      "\u001b[34mlr_config = dict(policy='step', step=[40, 80])\u001b[0m\n",
      "\u001b[34mtotal_epochs = 1\u001b[0m\n",
      "\u001b[34mcheckpoint_config = dict(interval=10)\u001b[0m\n",
      "\u001b[34mevaluation = dict(\n",
      "    interval=5,\n",
      "    metrics=['top_k_accuracy', 'mean_class_accuracy'],\n",
      "    gpu_collect=True)\u001b[0m\n",
      "\u001b[34mlog_config = dict(interval=5, hooks=[dict(type='TextLoggerHook')])\u001b[0m\n",
      "\u001b[34mdist_params = dict(backend='nccl')\u001b[0m\n",
      "\u001b[34mlog_level = 'INFO'\u001b[0m\n",
      "\u001b[34mwork_dir = './tutorial_exps'\u001b[0m\n",
      "\u001b[34mload_from = '/opt/ml/code/mmaction2/./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\u001b[0m\n",
      "\u001b[34mresume_from = None\u001b[0m\n",
      "\u001b[34mworkflow = [('train', 1)]\u001b[0m\n",
      "\u001b[34momnisource = False\u001b[0m\n",
      "\u001b[34mseed = 0\u001b[0m\n",
      "\u001b[34mgpu_ids = range(0, 1)\n",
      "\u001b[0m\n",
      "\u001b[34mFollowing command will be executed: \n",
      " python -m torch.distributed.launch --nnodes 2 --node_rank 0 --nproc_per_node 4 --master_addr algo-1 --master_port 55555 /opt/ml/code/mmaction2/tools/train.py /opt/ml/code/updated_config.py --launcher pytorch --work-dir /opt/ml/output/data\u001b[0m\n",
      "\u001b[35mStarting training...\u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[35mConfig:\u001b[0m\n",
      "\u001b[35mmodel = dict(\n",
      "    type='Recognizer2D',\n",
      "    backbone=dict(\n",
      "        type='ResNet',\n",
      "        pretrained='torchvision://resnet50',\n",
      "        depth=50,\n",
      "        norm_eval=False),\n",
      "    cls_head=dict(\n",
      "        type='TSNHead',\n",
      "        num_classes=2,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        consensus=dict(type='AvgConsensus', dim=1),\n",
      "        dropout_ratio=0.4,\n",
      "        init_std=0.01))\u001b[0m\n",
      "\u001b[35mtrain_cfg = None\u001b[0m\n",
      "\u001b[35mtest_cfg = dict(average_clips=None)\u001b[0m\n",
      "\u001b[35mdataset_type = 'VideoDataset'\u001b[0m\n",
      "\u001b[35mdata_root = '/opt/ml/input/data/training/train/'\u001b[0m\n",
      "\u001b[35mdata_root_val = '/opt/ml/input/data/training/val/'\u001b[0m\n",
      "\u001b[35mann_file_train = '/opt/ml/input/data/training/kinetics_tiny_train_video.txt'\u001b[0m\n",
      "\u001b[35mann_file_val = '/opt/ml/input/data/training/kinetics_tiny_val_video.txt'\u001b[0m\n",
      "\u001b[35mann_file_test = 'kinetics400_tiny/kinetics_tiny_val_video.txt'\u001b[0m\n",
      "\u001b[35mimg_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\u001b[0m\n",
      "\u001b[35mtrain_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=8),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(\n",
      "        type='MultiScaleCrop',\n",
      "        input_size=224,\n",
      "        scales=(1, 0.875, 0.75, 0.66),\n",
      "        random_crop=False,\n",
      "        max_wh_scale_gap=1),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\u001b[0m\n",
      "\u001b[35m]\u001b[0m\n",
      "\u001b[35mval_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=1,\n",
      "        frame_interval=1,\n",
      "        num_clips=8,\n",
      "        test_mode=True),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(type='Flip', flip_ratio=0),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\u001b[0m\n",
      "\u001b[35m]\u001b[0m\n",
      "\u001b[35mtest_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=1,\n",
      "        frame_interval=1,\n",
      "        num_clips=25,\n",
      "        test_mode=True),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(type='Flip', flip_ratio=0),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\u001b[0m\n",
      "\u001b[35m]\u001b[0m\n",
      "\u001b[35mdata = dict(\n",
      "    videos_per_gpu=2,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='/opt/ml/input/data/training/kinetics_tiny_train_video.txt',\n",
      "        data_prefix='/opt/ml/input/data/training/train/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames', clip_len=1, frame_interval=1,\n",
      "                num_clips=8),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(\n",
      "                type='MultiScaleCrop',\n",
      "                input_size=224,\n",
      "                scales=(1, 0.875, 0.75, 0.66),\n",
      "                random_crop=False,\n",
      "                max_wh_scale_gap=1),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='/opt/ml/input/data/training/kinetics_tiny_val_video.txt',\n",
      "        data_prefix='/opt/ml/input/data/training/val/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=1,\n",
      "                frame_interval=1,\n",
      "                num_clips=8,\n",
      "                test_mode=True),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(type='Flip', flip_ratio=0),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ]),\n",
      "    test=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='/opt/ml/input/data/training/kinetics_tiny_val_video.txt',\n",
      "        data_prefix='/opt/ml/input/data/training/val/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=1,\n",
      "                frame_interval=1,\n",
      "                num_clips=25,\n",
      "                test_mode=True),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(type='Flip', flip_ratio=0),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ]))\u001b[0m\n",
      "\u001b[35moptimizer = dict(type='SGD', lr=0.08, momentum=0.9, weight_decay=0.0001)\u001b[0m\n",
      "\u001b[35moptimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\u001b[0m\n",
      "\u001b[35mlr_config = dict(policy='step', step=[40, 80])\u001b[0m\n",
      "\u001b[35mtotal_epochs = 1\u001b[0m\n",
      "\u001b[35mcheckpoint_config = dict(interval=10)\u001b[0m\n",
      "\u001b[35mevaluation = dict(\n",
      "    interval=5,\n",
      "    metrics=['top_k_accuracy', 'mean_class_accuracy'],\n",
      "    gpu_collect=True)\u001b[0m\n",
      "\u001b[35mlog_config = dict(interval=5, hooks=[dict(type='TextLoggerHook')])\u001b[0m\n",
      "\u001b[35mdist_params = dict(backend='nccl')\u001b[0m\n",
      "\u001b[35mlog_level = 'INFO'\u001b[0m\n",
      "\u001b[35mwork_dir = './tutorial_exps'\u001b[0m\n",
      "\u001b[35mload_from = '/opt/ml/code/mmaction2/./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\u001b[0m\n",
      "\u001b[35mresume_from = None\u001b[0m\n",
      "\u001b[35mworkflow = [('train', 1)]\u001b[0m\n",
      "\u001b[35momnisource = False\u001b[0m\n",
      "\u001b[35mseed = 0\u001b[0m\n",
      "\u001b[35mgpu_ids = range(0, 1)\n",
      "\u001b[0m\n",
      "\u001b[35mFollowing command will be executed: \n",
      " python -m torch.distributed.launch --nnodes 2 --node_rank 1 --nproc_per_node 4 --master_addr algo-1 --master_port 55555 /opt/ml/code/mmaction2/tools/train.py /opt/ml/code/updated_config.py --launcher pytorch --work-dir /opt/ml/output/data\u001b[0m\n",
      "\u001b[35m*****************************************\u001b[0m\n",
      "\u001b[35mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\u001b[0m\n",
      "\u001b[35m*****************************************\u001b[0m\n",
      "\n",
      "2020-12-10 13:41:30 Uploading - Uploading generated training model\u001b[34mNCCL version 2.7.8+cuda10.2\u001b[0m\n",
      "\u001b[34mfatal: Not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: Not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: Not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: Not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34m2020-12-10 13:41:23,240 - mmaction - INFO - Environment info:\u001b[0m\n",
      "\u001b[34m------------------------------------------------------------\u001b[0m\n",
      "\u001b[34msys.platform: linux\u001b[0m\n",
      "\u001b[34mPython: 3.6.6 |Anaconda, Inc.| (default, Oct  9 2018, 12:34:16) [GCC 7.3.0]\u001b[0m\n",
      "\u001b[34mCUDA available: True\u001b[0m\n",
      "\u001b[34mGPU 0,1,2,3: Tesla V100-SXM2-16GB\u001b[0m\n",
      "\u001b[34mCUDA_HOME: /usr/local/cuda\u001b[0m\n",
      "\u001b[34mNVCC: Cuda compilation tools, release 10.1, V10.1.243\u001b[0m\n",
      "\u001b[34mGCC: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\u001b[0m\n",
      "\u001b[34mPyTorch: 1.7.0\u001b[0m\n",
      "\u001b[34mPyTorch compiling details: PyTorch built with:\u001b[0m\n",
      "\u001b[34m- GCC 7.3\u001b[0m\n",
      "\u001b[34m- C++ Version: 201402\u001b[0m\n",
      "\u001b[34m- Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\u001b[0m\n",
      "\u001b[34m- Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)\u001b[0m\n",
      "\u001b[34m- OpenMP 201511 (a.k.a. OpenMP 4.5)\u001b[0m\n",
      "\u001b[34m- NNPACK is enabled\u001b[0m\n",
      "\u001b[34m- CPU capability usage: AVX2\u001b[0m\n",
      "\u001b[34m- CUDA Runtime 10.2\u001b[0m\n",
      "\u001b[34m- NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75\u001b[0m\n",
      "\u001b[34m- CuDNN 7.6.5\u001b[0m\n",
      "\u001b[34m- Magma 2.5.2\u001b[0m\n",
      "\u001b[34m- Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON,\n",
      "\u001b[0m\n",
      "\u001b[34mTorchVision: 0.8.1\u001b[0m\n",
      "\u001b[34mOpenCV: 4.4.0\u001b[0m\n",
      "\u001b[34mMMCV: 1.2.1\u001b[0m\n",
      "\u001b[34mMMCV Compiler: GCC 7.3\u001b[0m\n",
      "\u001b[34mMMCV CUDA Compiler: 10.1\u001b[0m\n",
      "\u001b[34mMMAction2: 0.9.0+\u001b[0m\n",
      "\u001b[34m------------------------------------------------------------\n",
      "\u001b[0m\n",
      "\u001b[34m2020-12-10 13:41:23,241 - mmaction - INFO - Distributed training: True\u001b[0m\n",
      "\u001b[34m2020-12-10 13:41:23,241 - mmaction - INFO - Config: /opt/ml/code/updated_config.py\u001b[0m\n",
      "\u001b[34mmodel = dict(\u001b[0m\n",
      "\u001b[34mtype='Recognizer2D',\u001b[0m\n",
      "\u001b[34mbackbone=dict(\u001b[0m\n",
      "\u001b[34mtype='ResNet',\u001b[0m\n",
      "\u001b[34mpretrained='torchvision://resnet50',\u001b[0m\n",
      "\u001b[34mdepth=50,\u001b[0m\n",
      "\u001b[34mnorm_eval=False),\u001b[0m\n",
      "\u001b[34mcls_head=dict(\u001b[0m\n",
      "\u001b[34mtype='TSNHead',\u001b[0m\n",
      "\u001b[34mnum_classes=2,\u001b[0m\n",
      "\u001b[34min_channels=2048,\u001b[0m\n",
      "\u001b[34mspatial_type='avg',\u001b[0m\n",
      "\u001b[34mconsensus=dict(type='AvgConsensus', dim=1),\u001b[0m\n",
      "\u001b[34mdropout_ratio=0.4,\u001b[0m\n",
      "\u001b[34minit_std=0.01))\u001b[0m\n",
      "\u001b[34mtrain_cfg = None\u001b[0m\n",
      "\u001b[34mtest_cfg = dict(average_clips=None)\u001b[0m\n",
      "\u001b[34mdataset_type = 'VideoDataset'\u001b[0m\n",
      "\u001b[34mdata_root = '/opt/ml/input/data/training/train/'\u001b[0m\n",
      "\u001b[34mdata_root_val = '/opt/ml/input/data/training/val/'\u001b[0m\n",
      "\u001b[34mann_file_train = '/opt/ml/input/data/training/kinetics_tiny_train_video.txt'\u001b[0m\n",
      "\u001b[34mann_file_val = '/opt/ml/input/data/training/kinetics_tiny_val_video.txt'\u001b[0m\n",
      "\u001b[34mann_file_test = 'kinetics400_tiny/kinetics_tiny_val_video.txt'\u001b[0m\n",
      "\u001b[34mimg_norm_cfg = dict(\u001b[0m\n",
      "\u001b[34mmean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\u001b[0m\n",
      "\u001b[34mtrain_pipeline = [\u001b[0m\n",
      "\u001b[34mdict(type='DecordInit'),\u001b[0m\n",
      "\u001b[34mdict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=8),\u001b[0m\n",
      "\u001b[34mdict(type='DecordDecode'),\u001b[0m\n",
      "\u001b[34mdict(\u001b[0m\n",
      "\u001b[34mtype='MultiScaleCrop',\u001b[0m\n",
      "\u001b[34minput_size=224,\u001b[0m\n",
      "\u001b[34mscales=(1, 0.875, 0.75, 0.66),\u001b[0m\n",
      "\u001b[34mrandom_crop=False,\u001b[0m\n",
      "\u001b[34mmax_wh_scale_gap=1),\u001b[0m\n",
      "\u001b[34mdict(type='Resize', scale=(224, 224), keep_ratio=False),\u001b[0m\n",
      "\u001b[34mdict(type='Flip', flip_ratio=0.5),\u001b[0m\n",
      "\u001b[34mdict(\u001b[0m\n",
      "\u001b[34mtype='Normalize',\u001b[0m\n",
      "\u001b[34mmean=[123.675, 116.28, 103.53],\u001b[0m\n",
      "\u001b[34mstd=[58.395, 57.12, 57.375],\u001b[0m\n",
      "\u001b[34mto_bgr=False),\u001b[0m\n",
      "\u001b[34mdict(type='FormatShape', input_format='NCHW'),\u001b[0m\n",
      "\u001b[34mdict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\u001b[0m\n",
      "\u001b[34mdict(type='ToTensor', keys=['imgs', 'label'])\u001b[0m\n",
      "\u001b[34m]\u001b[0m\n",
      "\u001b[34mval_pipeline = [\u001b[0m\n",
      "\u001b[34mdict(type='DecordInit'),\u001b[0m\n",
      "\u001b[34mdict(\u001b[0m\n",
      "\u001b[34mtype='SampleFrames',\u001b[0m\n",
      "\u001b[34mclip_len=1,\u001b[0m\n",
      "\u001b[34mframe_interval=1,\u001b[0m\n",
      "\u001b[34mnum_clips=8,\u001b[0m\n",
      "\u001b[34mtest_mode=True),\u001b[0m\n",
      "\u001b[34mdict(type='DecordDecode'),\u001b[0m\n",
      "\u001b[34mdict(type='Resize', scale=(-1, 256)),\u001b[0m\n",
      "\u001b[34mdict(type='CenterCrop', crop_size=224),\u001b[0m\n",
      "\u001b[34mdict(type='Flip', flip_ratio=0),\u001b[0m\n",
      "\u001b[34mdict(\u001b[0m\n",
      "\u001b[34mtype='Normalize',\u001b[0m\n",
      "\u001b[34mmean=[123.675, 116.28, 103.53],\u001b[0m\n",
      "\u001b[34mstd=[58.395, 57.12, 57.375],\u001b[0m\n",
      "\u001b[34mto_bgr=False),\u001b[0m\n",
      "\u001b[34mdict(type='FormatShape', input_format='NCHW'),\u001b[0m\n",
      "\u001b[34mdict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\u001b[0m\n",
      "\u001b[34mdict(type='ToTensor', keys=['imgs'])\u001b[0m\n",
      "\u001b[34m]\u001b[0m\n",
      "\u001b[34mtest_pipeline = [\u001b[0m\n",
      "\u001b[34mdict(type='DecordInit'),\u001b[0m\n",
      "\u001b[34mdict(\u001b[0m\n",
      "\u001b[34mtype='SampleFrames',\u001b[0m\n",
      "\u001b[34mclip_len=1,\u001b[0m\n",
      "\u001b[34mframe_interval=1,\u001b[0m\n",
      "\u001b[34mnum_clips=25,\u001b[0m\n",
      "\u001b[34mtest_mode=True),\u001b[0m\n",
      "\u001b[34mdict(type='DecordDecode'),\u001b[0m\n",
      "\u001b[34mdict(type='Resize', scale=(-1, 256)),\u001b[0m\n",
      "\u001b[34mdict(type='ThreeCrop', crop_size=256),\u001b[0m\n",
      "\u001b[34mdict(type='Flip', flip_ratio=0),\u001b[0m\n",
      "\u001b[34mdict(\u001b[0m\n",
      "\u001b[34mtype='Normalize',\u001b[0m\n",
      "\u001b[34mmean=[123.675, 116.28, 103.53],\u001b[0m\n",
      "\u001b[34mstd=[58.395, 57.12, 57.375],\u001b[0m\n",
      "\u001b[34mto_bgr=False),\u001b[0m\n",
      "\u001b[34mdict(type='FormatShape', input_format='NCHW'),\u001b[0m\n",
      "\u001b[34mdict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\u001b[0m\n",
      "\u001b[34mdict(type='ToTensor', keys=['imgs'])\u001b[0m\n",
      "\u001b[34m]\u001b[0m\n",
      "\u001b[34mdata = dict(\u001b[0m\n",
      "\u001b[34mvideos_per_gpu=2,\u001b[0m\n",
      "\u001b[34mworkers_per_gpu=4,\u001b[0m\n",
      "\u001b[34mtrain=dict(\u001b[0m\n",
      "\u001b[34mtype='VideoDataset',\u001b[0m\n",
      "\u001b[34mann_file='/opt/ml/input/data/training/kinetics_tiny_train_video.txt',\u001b[0m\n",
      "\u001b[34mdata_prefix='/opt/ml/input/data/training/train/',\u001b[0m\n",
      "\u001b[34mpipeline=[\u001b[0m\n",
      "\u001b[34mdict(type='DecordInit'),\u001b[0m\n",
      "\u001b[34mdict(\u001b[0m\n",
      "\u001b[34mtype='SampleFrames', clip_len=1, frame_interval=1,\u001b[0m\n",
      "\u001b[34mnum_clips=8),\u001b[0m\n",
      "\u001b[34mdict(type='DecordDecode'),\u001b[0m\n",
      "\u001b[34mdict(\u001b[0m\n",
      "\u001b[34mtype='MultiScaleCrop',\u001b[0m\n",
      "\u001b[34minput_size=224,\u001b[0m\n",
      "\u001b[34mscales=(1, 0.875, 0.75, 0.66),\u001b[0m\n",
      "\u001b[34mrandom_crop=False,\u001b[0m\n",
      "\u001b[34mmax_wh_scale_gap=1),\u001b[0m\n",
      "\u001b[34mdict(type='Resize', scale=(224, 224), keep_ratio=False),\u001b[0m\n",
      "\u001b[34mdict(type='Flip', flip_ratio=0.5),\u001b[0m\n",
      "\u001b[34mdict(\u001b[0m\n",
      "\u001b[34mtype='Normalize',\u001b[0m\n",
      "\u001b[34mmean=[123.675, 116.28, 103.53],\u001b[0m\n",
      "\u001b[34mstd=[58.395, 57.12, 57.375],\u001b[0m\n",
      "\u001b[34mto_bgr=False),\u001b[0m\n",
      "\u001b[34mdict(type='FormatShape', input_format='NCHW'),\u001b[0m\n",
      "\u001b[34mdict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\u001b[0m\n",
      "\u001b[34mdict(type='ToTensor', keys=['imgs', 'label'])\u001b[0m\n",
      "\u001b[34m]),\u001b[0m\n",
      "\u001b[34mval=dict(\u001b[0m\n",
      "\u001b[34mtype='VideoDataset',\u001b[0m\n",
      "\u001b[34mann_file='/opt/ml/input/data/training/kinetics_tiny_val_video.txt',\u001b[0m\n",
      "\u001b[34mdata_prefix='/opt/ml/input/data/training/val/',\u001b[0m\n",
      "\u001b[34mpipeline=[\u001b[0m\n",
      "\u001b[34mdict(type='DecordInit'),\u001b[0m\n",
      "\u001b[34mdict(\u001b[0m\n",
      "\u001b[34mtype='SampleFrames',\u001b[0m\n",
      "\u001b[34mclip_len=1,\u001b[0m\n",
      "\u001b[34mframe_interval=1,\u001b[0m\n",
      "\u001b[34mnum_clips=8,\u001b[0m\n",
      "\u001b[34mtest_mode=True),\u001b[0m\n",
      "\u001b[34mdict(type='DecordDecode'),\u001b[0m\n",
      "\u001b[34mdict(type='Resize', scale=(-1, 256)),\u001b[0m\n",
      "\u001b[34mdict(type='CenterCrop', crop_size=224),\u001b[0m\n",
      "\u001b[34mdict(type='Flip', flip_ratio=0),\u001b[0m\n",
      "\u001b[34mdict(\u001b[0m\n",
      "\u001b[34mtype='Normalize',\u001b[0m\n",
      "\u001b[34mmean=[123.675, 116.28, 103.53],\u001b[0m\n",
      "\u001b[34mstd=[58.395, 57.12, 57.375],\u001b[0m\n",
      "\u001b[34mto_bgr=False),\u001b[0m\n",
      "\u001b[34mdict(type='FormatShape', input_format='NCHW'),\u001b[0m\n",
      "\u001b[34mdict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\u001b[0m\n",
      "\u001b[34mdict(type='ToTensor', keys=['imgs'])\u001b[0m\n",
      "\u001b[34m]),\u001b[0m\n",
      "\u001b[34mtest=dict(\u001b[0m\n",
      "\u001b[34mtype='VideoDataset',\u001b[0m\n",
      "\u001b[34mann_file='/opt/ml/input/data/training/kinetics_tiny_val_video.txt',\u001b[0m\n",
      "\u001b[34mdata_prefix='/opt/ml/input/data/training/val/',\u001b[0m\n",
      "\u001b[34mpipeline=[\u001b[0m\n",
      "\u001b[34mdict(type='DecordInit'),\u001b[0m\n",
      "\u001b[34mdict(\u001b[0m\n",
      "\u001b[34mtype='SampleFrames',\u001b[0m\n",
      "\u001b[34mclip_len=1,\u001b[0m\n",
      "\u001b[34mframe_interval=1,\u001b[0m\n",
      "\u001b[34mnum_clips=25,\u001b[0m\n",
      "\u001b[34mtest_mode=True),\u001b[0m\n",
      "\u001b[34mdict(type='DecordDecode'),\u001b[0m\n",
      "\u001b[34mdict(type='Resize', scale=(-1, 256)),\u001b[0m\n",
      "\u001b[34mdict(type='ThreeCrop', crop_size=256),\u001b[0m\n",
      "\u001b[34mdict(type='Flip', flip_ratio=0),\u001b[0m\n",
      "\u001b[34mdict(\u001b[0m\n",
      "\u001b[34mtype='Normalize',\u001b[0m\n",
      "\u001b[34mmean=[123.675, 116.28, 103.53],\u001b[0m\n",
      "\u001b[34mstd=[58.395, 57.12, 57.375],\u001b[0m\n",
      "\u001b[34mto_bgr=False),\u001b[0m\n",
      "\u001b[34mdict(type='FormatShape', input_format='NCHW'),\u001b[0m\n",
      "\u001b[34mdict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\u001b[0m\n",
      "\u001b[34mdict(type='ToTensor', keys=['imgs'])\u001b[0m\n",
      "\u001b[34m]))\u001b[0m\n",
      "\u001b[34moptimizer = dict(type='SGD', lr=0.08, momentum=0.9, weight_decay=0.0001)\u001b[0m\n",
      "\u001b[34moptimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\u001b[0m\n",
      "\u001b[34mlr_config = dict(policy='step', step=[40, 80])\u001b[0m\n",
      "\u001b[34mtotal_epochs = 1\u001b[0m\n",
      "\u001b[34mcheckpoint_config = dict(interval=10)\u001b[0m\n",
      "\u001b[34mevaluation = dict(\u001b[0m\n",
      "\u001b[34minterval=5,\u001b[0m\n",
      "\u001b[34mmetrics=['top_k_accuracy', 'mean_class_accuracy'],\u001b[0m\n",
      "\u001b[34mgpu_collect=True)\u001b[0m\n",
      "\u001b[34mlog_config = dict(interval=5, hooks=[dict(type='TextLoggerHook')])\u001b[0m\n",
      "\u001b[34mdist_params = dict(backend='nccl')\u001b[0m\n",
      "\u001b[34mlog_level = 'INFO'\u001b[0m\n",
      "\u001b[34mwork_dir = './tutorial_exps'\u001b[0m\n",
      "\u001b[34mload_from = '/opt/ml/code/mmaction2/./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\u001b[0m\n",
      "\u001b[34mresume_from = None\u001b[0m\n",
      "\u001b[34mworkflow = [('train', 1)]\u001b[0m\n",
      "\u001b[34momnisource = False\u001b[0m\n",
      "\u001b[34mseed = 0\u001b[0m\n",
      "\u001b[34mgpu_ids = range(0, 1)\n",
      "\u001b[0m\n",
      "\u001b[35mfatal: Not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: Not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: Not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: Not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mDownloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\u001b[0m\n",
      "\u001b[34m0%|          | 0.00/97.8M [00:00<?, ?B/s]#015 19%|█▉        | 18.8M/97.8M [00:00<00:00, 197MB/s]#015 39%|███▉      | 38.0M/97.8M [00:00<00:00, 198MB/s]#015 58%|█████▊    | 56.8M/97.8M [00:00<00:00, 198MB/s]#015 77%|███████▋  | 75.4M/97.8M [00:00<00:00, 197MB/s]#015 96%|█████████▌| 94.1M/97.8M [00:00<00:00, 197MB/s]#015100%|██████████| 97.8M/97.8M [00:00<00:00, 197MB/s]\u001b[0m\n",
      "\u001b[35mDownloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\u001b[0m\n",
      "\u001b[34m2020-12-10 13:41:25,248 - mmaction - INFO - These parameters in pretrained checkpoint are not loaded: {'fc.bias', 'fc.weight'}\u001b[0m\n",
      "\u001b[34mfatal: Not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: Not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: Not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: Not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34m2020-12-10 13:41:25,606 - mmaction - INFO - load checkpoint from /opt/ml/code/mmaction2/./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/mmaction2/tools/train.py\", line 178, in <module>\u001b[0m\n",
      "\u001b[34mmain()\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/mmaction2/tools/train.py\", line 174, in main\u001b[0m\n",
      "\u001b[34mmeta=meta)\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/mmaction2/mmaction/apis/train.py\", line 137, in train_model\u001b[0m\n",
      "\u001b[34mrunner.load_checkpoint(cfg.load_from)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/base_runner.py\", line 312, in load_checkpoint\u001b[0m\n",
      "\u001b[34mself.logger)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/checkpoint.py\", line 247, in load_checkpoint\u001b[0m\n",
      "\u001b[34mcheckpoint = _load_checkpoint(filename, map_location)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/checkpoint.py\", line 222, in _load_checkpoint\u001b[0m\n",
      "\u001b[34mraise IOError(f'{filename} is not a checkpoint file')\u001b[0m\n",
      "\u001b[34mOSError: /opt/ml/code/mmaction2/./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth is not a checkpoint file\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/mmaction2/tools/train.py\", line 178, in <module>\u001b[0m\n",
      "\u001b[34mmain()\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/mmaction2/tools/train.py\", line 174, in main\u001b[0m\n",
      "\u001b[34mmeta=meta)\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/mmaction2/mmaction/apis/train.py\", line 137, in train_model\u001b[0m\n",
      "\u001b[34mrunner.load_checkpoint(cfg.load_from)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/base_runner.py\", line 312, in load_checkpoint\u001b[0m\n",
      "\u001b[34mself.logger)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/checkpoint.py\", line 247, in load_checkpoint\u001b[0m\n",
      "\u001b[34mcheckpoint = _load_checkpoint(filename, map_location)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/checkpoint.py\", line 222, in _load_checkpoint\u001b[0m\n",
      "\u001b[34mraise IOError(f'{filename} is not a checkpoint file')\u001b[0m\n",
      "\u001b[34mOSError: /opt/ml/code/mmaction2/./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth is not a checkpoint file\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/mmaction2/tools/train.py\", line 178, in <module>\u001b[0m\n",
      "\u001b[34mmain()\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/mmaction2/tools/train.py\", line 174, in main\u001b[0m\n",
      "\u001b[34mmeta=meta)\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/mmaction2/mmaction/apis/train.py\", line 137, in train_model\u001b[0m\n",
      "\u001b[34mrunner.load_checkpoint(cfg.load_from)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/base_runner.py\", line 312, in load_checkpoint\u001b[0m\n",
      "\u001b[34mself.logger)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/checkpoint.py\", line 247, in load_checkpoint\u001b[0m\n",
      "\u001b[34mcheckpoint = _load_checkpoint(filename, map_location)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/checkpoint.py\", line 222, in _load_checkpoint\u001b[0m\n",
      "\u001b[34mraise IOError(f'{filename} is not a checkpoint file')\u001b[0m\n",
      "\u001b[34mOSError: /opt/ml/code/mmaction2/./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth is not a checkpoint file\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/mmaction2/tools/train.py\", line 178, in <module>\u001b[0m\n",
      "\u001b[34mmain()\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/mmaction2/tools/train.py\", line 174, in main\u001b[0m\n",
      "\u001b[34mmeta=meta)\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/mmaction2/mmaction/apis/train.py\", line 137, in train_model\u001b[0m\n",
      "\u001b[34mrunner.load_checkpoint(cfg.load_from)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/base_runner.py\", line 312, in load_checkpoint\u001b[0m\n",
      "\u001b[34mself.logger)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/checkpoint.py\", line 247, in load_checkpoint\u001b[0m\n",
      "\u001b[34mcheckpoint = _load_checkpoint(filename, map_location)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/checkpoint.py\", line 222, in _load_checkpoint\u001b[0m\n",
      "\u001b[34mraise IOError(f'{filename} is not a checkpoint file')\u001b[0m\n",
      "\u001b[34mOSError: /opt/ml/code/mmaction2/./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth is not a checkpoint file\u001b[0m\n",
      "\u001b[35m0%|          | 0.00/97.8M [00:00<?, ?B/s]#015 11%|█         | 10.9M/97.8M [00:00<00:00, 114MB/s]#015 20%|█▉        | 19.4M/97.8M [00:00<00:00, 105MB/s]#015 29%|██▊       | 28.1M/97.8M [00:00<00:00, 100MB/s]#015 38%|███▊      | 37.1M/97.8M [00:00<00:00, 98.5MB/s]#015 47%|████▋     | 46.1M/97.8M [00:00<00:00, 97.1MB/s]#015 56%|█████▌    | 54.8M/97.8M [00:00<00:00, 95.4MB/s]#015 65%|██████▌   | 63.8M/97.8M [00:00<00:00, 95.1MB/s]#015 74%|███████▍  | 72.5M/97.8M [00:00<00:00, 93.8MB/s]#015 84%|████████▎ | 81.8M/97.8M [00:00<00:00, 94.8MB/s]#015 93%|█████████▎| 90.9M/97.8M [00:01<00:00, 94.7MB/s]#015100%|██████████| 97.8M/97.8M [00:01<00:00, 94.8MB/s]\u001b[0m\n",
      "\u001b[35mfatal: Not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: Not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: Not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: Not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[35mFile \"/opt/ml/code/mmaction2/tools/train.py\", line 178, in <module>\u001b[0m\n",
      "\u001b[35mFile \"/opt/ml/code/mmaction2/tools/train.py\", line 178, in <module>\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[35mFile \"/opt/ml/code/mmaction2/tools/train.py\", line 178, in <module>\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[35mmain()\u001b[0m\n",
      "\u001b[35mmain()\u001b[0m\n",
      "\u001b[35mFile \"/opt/ml/code/mmaction2/tools/train.py\", line 174, in main\u001b[0m\n",
      "\u001b[35mFile \"/opt/ml/code/mmaction2/tools/train.py\", line 174, in main\u001b[0m\n",
      "\u001b[35mFile \"/opt/ml/code/mmaction2/tools/train.py\", line 178, in <module>\u001b[0m\n",
      "\u001b[35mmain()\u001b[0m\n",
      "\u001b[35mFile \"/opt/ml/code/mmaction2/tools/train.py\", line 174, in main\u001b[0m\n",
      "\u001b[35mmeta=meta)\u001b[0m\n",
      "\u001b[35mmeta=meta)\u001b[0m\n",
      "\u001b[35mFile \"/opt/ml/code/mmaction2/mmaction/apis/train.py\", line 137, in train_model\u001b[0m\n",
      "\u001b[35mFile \"/opt/ml/code/mmaction2/mmaction/apis/train.py\", line 137, in train_model\u001b[0m\n",
      "\u001b[35mmeta=meta)\u001b[0m\n",
      "\u001b[35mFile \"/opt/ml/code/mmaction2/mmaction/apis/train.py\", line 137, in train_model\u001b[0m\n",
      "\u001b[35mmain()\u001b[0m\n",
      "\u001b[35mrunner.load_checkpoint(cfg.load_from)\u001b[0m\n",
      "\u001b[35mrunner.load_checkpoint(cfg.load_from)\u001b[0m\n",
      "\u001b[35mFile \"/opt/ml/code/mmaction2/tools/train.py\", line 174, in main\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/base_runner.py\", line 312, in load_checkpoint\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/base_runner.py\", line 312, in load_checkpoint\u001b[0m\n",
      "\u001b[35mrunner.load_checkpoint(cfg.load_from)\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/base_runner.py\", line 312, in load_checkpoint\u001b[0m\n",
      "\u001b[35mmeta=meta)\u001b[0m\n",
      "\u001b[35mFile \"/opt/ml/code/mmaction2/mmaction/apis/train.py\", line 137, in train_model\u001b[0m\n",
      "\u001b[35mrunner.load_checkpoint(cfg.load_from)\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/base_runner.py\", line 312, in load_checkpoint\u001b[0m\n",
      "\u001b[35mself.logger)\u001b[0m\n",
      "\u001b[35mself.logger)\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/checkpoint.py\", line 247, in load_checkpoint\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/checkpoint.py\", line 247, in load_checkpoint\u001b[0m\n",
      "\u001b[35mself.logger)\u001b[0m\n",
      "\u001b[35mself.logger)\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/checkpoint.py\", line 247, in load_checkpoint\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/checkpoint.py\", line 247, in load_checkpoint\u001b[0m\n",
      "\u001b[35mcheckpoint = _load_checkpoint(filename, map_location)\u001b[0m\n",
      "\u001b[35mcheckpoint = _load_checkpoint(filename, map_location)\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/checkpoint.py\", line 222, in _load_checkpoint\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/checkpoint.py\", line 222, in _load_checkpoint\u001b[0m\n",
      "\u001b[35mcheckpoint = _load_checkpoint(filename, map_location)\u001b[0m\n",
      "\u001b[35mcheckpoint = _load_checkpoint(filename, map_location)\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/checkpoint.py\", line 222, in _load_checkpoint\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/checkpoint.py\", line 222, in _load_checkpoint\u001b[0m\n",
      "\u001b[35mraise IOError(f'{filename} is not a checkpoint file')\u001b[0m\n",
      "\u001b[35mraise IOError(f'{filename} is not a checkpoint file')\u001b[0m\n",
      "\u001b[35mOSError: /opt/ml/code/mmaction2/./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth is not a checkpoint file\u001b[0m\n",
      "\u001b[35mOSError: /opt/ml/code/mmaction2/./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth is not a checkpoint file\u001b[0m\n",
      "\u001b[35mraise IOError(f'{filename} is not a checkpoint file')\u001b[0m\n",
      "\u001b[35mOSError: /opt/ml/code/mmaction2/./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth is not a checkpoint file\u001b[0m\n",
      "\u001b[35mraise IOError(f'{filename} is not a checkpoint file')\u001b[0m\n",
      "\u001b[35mOSError: /opt/ml/code/mmaction2/./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth is not a checkpoint file\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m\"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34mexec(code, run_globals)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py\", line 260, in <module>\u001b[0m\n",
      "\u001b[34mmain()\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py\", line 256, in main\u001b[0m\n",
      "\u001b[34mcmd=cmd)\u001b[0m\n",
      "\u001b[34msubprocess.CalledProcessError: Command '['/opt/conda/bin/python', '-u', '/opt/ml/code/mmaction2/tools/train.py', '--local_rank=3', '/opt/ml/code/updated_config.py', '--launcher', 'pytorch', '--work-dir', '/opt/ml/output/data']' returned non-zero exit status 1.\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[35m\"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[35mexec(code, run_globals)\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py\", line 260, in <module>\u001b[0m\n",
      "\u001b[35mmain()\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py\", line 256, in main\u001b[0m\n",
      "\u001b[35mcmd=cmd)\u001b[0m\n",
      "\u001b[35msubprocess.CalledProcessError: Command '['/opt/conda/bin/python', '-u', '/opt/ml/code/mmaction2/tools/train.py', '--local_rank=3', '/opt/ml/code/updated_config.py', '--launcher', 'pytorch', '--work-dir', '/opt/ml/output/data']' returned non-zero exit status 1.\u001b[0m\n",
      "\u001b[34m2020-12-10 13:41:26,960 sagemaker-containers ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python mmaction2_train.py --auto-scale False --config-file configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py --dataset kinetics400_tiny --options total_epochs=1; optimizer.lr=0.08; evaluation.gpu_collect=True --validate True\"\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"mmaction2_train.py\", line 267, in <module>\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=joint_cmd)\u001b[0m\n",
      "\u001b[34msubprocess.CalledProcessError: Command 'python -m torch.distributed.launch --nnodes 2 --node_rank 0 --nproc_per_node 4 --master_addr algo-1 --master_port 55555 /opt/ml/code/mmaction2/tools/train.py /opt/ml/code/updated_config.py --launcher pytorch --work-dir /opt/ml/output/data' returned non-zero exit status 1.\u001b[0m\n",
      "\u001b[35m2020-12-10 13:41:26,718 sagemaker-containers ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[35mCommand \"/opt/conda/bin/python mmaction2_train.py --auto-scale False --config-file configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py --dataset kinetics400_tiny --options total_epochs=1; optimizer.lr=0.08; evaluation.gpu_collect=True --validate True\"\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"mmaction2_train.py\", line 267, in <module>\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=joint_cmd)\u001b[0m\n",
      "\u001b[35msubprocess.CalledProcessError: Command 'python -m torch.distributed.launch --nnodes 2 --node_rank 1 --nproc_per_node 4 --master_addr algo-1 --master_port 55555 /opt/ml/code/mmaction2/tools/train.py /opt/ml/code/updated_config.py --launcher pytorch --work-dir /opt/ml/output/data' returned non-zero exit status 1.\u001b[0m\n",
      "\n",
      "2020-12-10 13:41:42 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job mmaction2-training-2020-12-10-13-33-32-464: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python mmaction2_train.py --auto-scale False --config-file configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py --dataset kinetics400_tiny --options total_epochs=1; optimizer.lr=0.08; evaluation.gpu_collect=True --validate True\"\nTraceback (most recent call last):\n  File \"mmaction2_train.py\", line 267, in <module>\n    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=joint_cmd)\nsubprocess.CalledProcessError: Command 'python -m torch.distributed.launch --nnodes 2 --node_rank 1 --nproc_per_node 4 --master_addr algo-1 --master_port 55555 /opt/ml/code/mmaction2/tools/train.py /opt/ml/code/updated_config.py --launcher pytorch --work-dir /opt/ml/output/data' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-84a06979ab47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"training\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m\"s3://\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/kinetics400_tiny/\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3680\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3681\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3682\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3683\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3259\u001b[0m                 ),\n\u001b[1;32m   3260\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3261\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3262\u001b[0m             )\n\u001b[1;32m   3263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job mmaction2-training-2020-12-10-13-33-32-464: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python mmaction2_train.py --auto-scale False --config-file configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py --dataset kinetics400_tiny --options total_epochs=1; optimizer.lr=0.08; evaluation.gpu_collect=True --validate True\"\nTraceback (most recent call last):\n  File \"mmaction2_train.py\", line 267, in <module>\n    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=joint_cmd)\nsubprocess.CalledProcessError: Command 'python -m torch.distributed.launch --nnodes 2 --node_rank 1 --nproc_per_node 4 --master_addr algo-1 --master_port 55555 /opt/ml/code/mmaction2/tools/train.py /opt/ml/code/updated_config.py --launcher pytorch --work-dir /opt/ml/output/data' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "est = sagemaker.estimator.Estimator(image,\n",
    "                                          role=role,\n",
    "                                          train_instance_count=2,\n",
    "                                          train_instance_type='ml.p3.8xlarge',\n",
    "                                          train_volume_size=100,\n",
    "                                          output_path=\"s3://{}/{}\".format(bucket, prefix_output),\n",
    "                                          metric_definitions = metrics,\n",
    "                                          hyperparameters = hyperparameters, \n",
    "                                          sagemaker_session=session\n",
    ")\n",
    "\n",
    "est.fit({\"training\" : \"s3://\"+bucket+\"/kinetics400_tiny/\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
